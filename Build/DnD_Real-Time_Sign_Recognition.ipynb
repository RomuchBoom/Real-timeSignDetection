{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e624163c",
   "metadata": {},
   "source": [
    "# 1) Install and Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe matplotlib numpy tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64981b1",
   "metadata": {},
   "source": [
    "# 2) Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0130f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path where data will be stored\n",
    "path = os.path.join('Data')\n",
    "\n",
    "# Create a list of sign names that best describe your gestures \n",
    "signs = ['Armor', 'Curse', 'Nothing', 'Psyonic', 'Shadow']\n",
    "\n",
    "# Create a list of augmentations that will be used on the original camera feed\n",
    "augmentations = ['Original', 'Saturation', 'Hue', 'Contrast', 'Brightness']\n",
    "\n",
    "# Set a number of examples for each sign\n",
    "len_sentence = 100\n",
    "\n",
    "# Set a number of frames for each example\n",
    "len_sequence = 30\n",
    "\n",
    "# Extract a number of predictable classes\n",
    "num_classes = len(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e537b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every sign, augmentation, and sentence number\n",
    "for sign in signs:\n",
    "    for augmentation in augmentations:\n",
    "        for sentence in range(1, len_sentence+1):\n",
    "            \n",
    "            # Make a path for an individual sentence (example)\n",
    "            sentence_path = os.path.join(path, sign, augmentation, str(sentence))\n",
    "                 \n",
    "            try:\n",
    "                \n",
    "                # Create a sentence_path directory if it does not exist \n",
    "                os.makedirs(sentence_path)\n",
    "\n",
    "            except OSError as error:\n",
    "                \n",
    "                # If the directory already exists it prints an error\n",
    "                print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906d362",
   "metadata": {},
   "source": [
    "# 3) Collect image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, holistic):\n",
    "    \"\"\"\n",
    "    Processes an image and detects facial, pose, and hand landmarks. \n",
    "    \n",
    "    Arguments:\n",
    "    image -- np.array, from which landmarks get detected \n",
    "    holistic -- MediaPipe Holistic model that detects landmarks from an image\n",
    "    \n",
    "    Returns:\n",
    "    landmarks -- output from holistic that contains face, pose, and hand position\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make our np.array immutable\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Convert image from BGR to RGB color order\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process image to get landmarks\n",
    "    landmarks = holistic.process(image)\n",
    "    \n",
    "    # Make an array mutable again\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Pulls out the hand landmarks only.\n",
    "    Normally, if a hand is in a frame, its landmarks should not be None.\n",
    "    If hand's landmarks were not detected, we set them all equal to zero.\n",
    "    \n",
    "    Arguments:\n",
    "    landmarks -- output from a holistic model with positions of face, pose, and hands\n",
    "    \n",
    "    Returns:\n",
    "    hands -- list of values for left and right hand landmarks [shape = (126, )]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get x,y and z values for every landmark out of 21 for each hand\n",
    "    # If landmarks are None - create an np.array filled with zeros (21 points with 3 coordinates)\n",
    "    right_hand = np.array([[l.x, l.y, l.z] for l in landmarks.right_hand_landmarks.landmark]).flatten() if not landmarks.right_hand_landmarks == None else np.zeros(21*3)    \n",
    "    left_hand = np.array([[l.x, l.y, l.z] for l in landmarks.left_hand_landmarks.landmark]).flatten() if not landmarks.left_hand_landmarks == None else np.zeros(21*3)\n",
    "    \n",
    "    hands = np.concatenate([right_hand, left_hand])\n",
    "    return hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    \"\"\"\n",
    "    Applies different augmentation techniques to image.\n",
    "    \n",
    "    Arguments:\n",
    "    image -- feed from webcam represented as np.array\n",
    "    \n",
    "    Returns:\n",
    "    augmented_images -- list, which contains the original image and its augmented version\n",
    "    \"\"\"\n",
    "    \n",
    "    saturated = np.array(tf.image.random_saturation(image, 0.75, 1.75))\n",
    "    hued = np.array(tf.image.random_hue(image, 0.1))\n",
    "    contrasted = np.array(tf.image.random_contrast(image, 0.75, 1.5))\n",
    "    brightnessed = np.array(tf.image.random_brightness(image, 0.2))\n",
    "    \n",
    "    # Create a list with all versions of the given image\n",
    "    augmented_images = [image, saturated, hued, contrasted, brightnessed]\n",
    "    \n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fe71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sign_data(path, sign, augmentations, len_sentence, len_sequence):\n",
    "    \"\"\"\n",
    "    Collects, processes, and saves an image dataset.\n",
    "    When particular key is pressed, it starts to collect every frame.\n",
    "    To every collected frame applies data augmentation techniques to expand the dataset.\n",
    "    When sentence is formed (collected len_sequence number of frames), it saves the data to the corresponding folder.\n",
    "    \n",
    "    Arguments:\n",
    "    path -- path, which indicates where to save the data\n",
    "    sign -- action name, for which data is collecting \n",
    "    augmentations -- list of augmentations to apply to an image\n",
    "    len_sentence -- number of examples for each sign\n",
    "    len_sequence -- number of frames for each sentence (example)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indicates when to start collecting the data\n",
    "    process = False\n",
    "    \n",
    "    # Checks when a number of examples is reached\n",
    "    sentence_cnt = 1\n",
    "    \n",
    "    # List to store the augmented images \n",
    "    images_sentence = []\n",
    "    \n",
    "    # Specify a webcam device\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set up the MediaPipe Holistic model\n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence=0.75, min_tracking_confidence=0.75) as holistic:\n",
    "\n",
    "        # Until the capturing device is not shut down and we did not reach necessary amount of sentences (examples)\n",
    "        while cap.isOpened() and sentence_cnt <= len_sentence:\n",
    "\n",
    "            # Get feed from the webcam\n",
    "            _, frame = cap.read()       \n",
    "            \n",
    "            # Start gathering the data when 'p' is pressed\n",
    "            if cv2.waitKey(10) & 0XFF == ord('p'):                   \n",
    "                process = True\n",
    "                    \n",
    "            \n",
    "            if process:\n",
    "                \n",
    "                # Augment the original image and save it with other augmented versions to the sentence list \n",
    "                images = augment_image(frame)\n",
    "                images_sentence.append(images) \n",
    "                \n",
    "                # When frames are currently collecting\n",
    "                if len(images_sentence) < len_sequence-1:\n",
    "                    cv2.putText(frame, \"Collecting samples [{} out of {}]\".format(sentence_cnt, len_sentence), \n",
    "                            (0, 20), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 171, 255), 2, cv2.LINE_AA)\n",
    "                     \n",
    "                else:\n",
    "                    cv2.putText(frame, \"Saving...\", (0, 20), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 171, 255), 2, cv2.LINE_AA)\n",
    "                  \n",
    "            # When gathered images are enough to form a sentence\n",
    "            if len(images_sentence) == len_sequence:\n",
    "                    \n",
    "                # Iterate through every augmented image and the corresponding augmentation name\n",
    "                for image, augmentation in zip(images, augmentations):\n",
    "                    \n",
    "                    # Get landmarks from an image\n",
    "                    landmarks = process_image(image, holistic)\n",
    "                    \n",
    "                    # Extract hand landmarks\n",
    "                    image_landmarks = extract_hand_landmarks(landmarks)\n",
    "                    \n",
    "                    # Create a path where to save the sentence of certain augmentation\n",
    "                    image_path = os.path.join(path, sign, augmentation, str(sentence_cnt), str(images_cnt+1))\n",
    "                    \n",
    "                    # Save the data using the specified path\n",
    "                    np.save(image_path, image_landmarks)\n",
    "                    \n",
    "                # Reset variables' values\n",
    "                sentence_cnt, process, images_sentence = sentence_cnt+1, False, []\n",
    "            \n",
    "            # Break from the while loop if necessary\n",
    "            if cv2.waitKey(10) & 0XFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "            # Show the frame in a window and an actual sign in a window's name \n",
    "            cv2.imshow(\"Sign - '{}'\".format(sign), frame)\n",
    "            \n",
    "            \n",
    "        # Release the capturing device and destroy the output window    \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every sign and collect the corresponding data\n",
    "for sign in signs:\n",
    "    collect_sign_data(path, sign, augmentations, len_sentence, len_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674102e0",
   "metadata": {},
   "source": [
    "# 4) Load image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store image data and labels\n",
    "features, labels = [], []\n",
    "\n",
    "# Iterate through every sentence (example)\n",
    "for label, sign in enumerate(os.listdir(path)):\n",
    "    for augmentation in augmentations:\n",
    "        for sentence in range(1, len_sentence+1):\n",
    "\n",
    "            # Create an empty list to store the sentence data\n",
    "            example = []\n",
    "            for sequence in range(1, len_sequence+1):\n",
    "\n",
    "                # Load every frame within the sentence and append it to the sentence list\n",
    "                frame = np.load(os.path.join(path, sign, augmentation, str(sentence), str(sequence) + '.npy')).flatten()\n",
    "                example.append(frame)\n",
    "\n",
    "            # Append every sentence and its label to the appropriate lists \n",
    "            features.append(example)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features list to the np.array type\n",
    "X = np.array(features)\n",
    "\n",
    "# Convert the labels list to the sparse matrix (in each column 1 equals to true label)\n",
    "Y = tfk.utils.to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19080556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92944254",
   "metadata": {},
   "source": [
    "# 5) Create Sign Recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras Sequential model\n",
    "model = tfk.Sequential()\n",
    "\n",
    "# Input Layer with the shape (30, 126), where 126 equals to number of hand landmarks (21 points * 3 coordinates * 2 hands) \n",
    "model.add(tfk.layers.InputLayer(input_shape=(len_sequence, 126)))\n",
    "\n",
    "# Long-Short Term Memory layer with applying dropout to prevent overfitting\n",
    "model.add(tfk.layers.LSTM(units=32, dropout=0.1, return_sequences=True))\n",
    "model.add(tfk.layers.LSTM(units=64, return_sequences=True))\n",
    "model.add(tfk.layers.LSTM(units=64, dropout=0.15, return_sequences=True))\n",
    "model.add(tfk.layers.LSTM(units=32, dropout=0.1, return_sequences=False))\n",
    "\n",
    "# Dense (Fully-connected) layer with 'relu' activation function\n",
    "model.add(tfk.layers.Dense(units=32, activation='relu'))\n",
    "model.add(tfk.layers.Dense(units=16, activation='relu'))\n",
    "\n",
    "# Dense layer with number of units equal to the number of signs (classes we want to predict) and 'softmax' activation function \n",
    "model.add(tfk.layers.Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's acrhitecture information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_decay(epoch, learning_rate):\n",
    "    \"\"\"\n",
    "    Decreases learning_rate when certain number of epochs is reached.\n",
    "    \n",
    "    Arguments:\n",
    "    epoch -- iteration number that train data was passed through the model when training\n",
    "    learning_rate -- value, which controls the length of 'steps' that model takes  \n",
    "    \n",
    "    Returns:\n",
    "    learning_rate -- appropriate epoch-dependent value of learning_rate\n",
    "    \"\"\"\n",
    "    if epoch % 9 == 0:\n",
    "        return learning_rate * 0.75\n",
    "    \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to apply the 'learning_rate_decay' function when training the model\n",
    "scheduler = tfk.callbacks.LearningRateScheduler(learning_rate_decay)\n",
    "\n",
    "# Set an optimizer to use when training the model\n",
    "optimizer = tfk.optimizers.Adam(learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6930edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group layers into trainable object with specific parameters\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cc4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the compiled model\n",
    "model.fit(X_train, Y_train, batch_size = 16, epochs=100, callbacks=[scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08174ca3",
   "metadata": {},
   "source": [
    "# 6) Save and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a path where the trained model will be saved\n",
    "model_path = 'model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da163e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(os.path.join(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tfk.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af509f",
   "metadata": {},
   "source": [
    "# 7) Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ytrue_yhat(X, Y, model):\n",
    "    \"\"\"\n",
    "    Calculates and transforms true and predicted labels.\n",
    "    Previously trained model is going to make predictions (determine labels) based on data X.\n",
    "    Labels Y are needed to get the index of the correct label from each instance.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Data, from which predictions are made\n",
    "    Y -- True labels assigned to each data example \n",
    "    model -- trained object that makes predictions\n",
    "    \n",
    "    Returns:\n",
    "    ytrue -- extracted indexes of true labels\n",
    "    yhat -- extracted indexes of predicted by model labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the model to predict labels using the provided data\n",
    "    yhat = model.predict(X)\n",
    "    \n",
    "    # Extract label indexes with the highest probability\n",
    "    ytrue = np.argmax(Y, axis = 1).tolist()\n",
    "    yhat = np.argmax(yhat, axis = 1).tolist()\n",
    "    \n",
    "    return ytrue, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(ytrue, yhat, num_classes):\n",
    "    \"\"\"\n",
    "    Creates and displays a confusion matrix to evaluate results.\n",
    "    \n",
    "    Arguments:\n",
    "    ytrue -- indexes of true lables\n",
    "    yhat -- indexes of predicted by model labels\n",
    "    num_classes -- number of classes that the model is trained to predict\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a confusion matrix with 5 classes\n",
    "    cm_train = confusion_matrix(ytrue, yhat, labels=np.arange(num_classes))\n",
    "    \n",
    "    # Create a visualization for the confusion matrix\n",
    "    cmd_train = ConfusionMatrixDisplay(cm_train)\n",
    "    \n",
    "    # Display the results\n",
    "    cmd_train.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d830f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ytrue and yhat based on the training data\n",
    "ytrue_train, yhat_train = get_ytrue_yhat(X_train, Y_train, model)\n",
    "\n",
    "# Visualize the confusion matrix to evaluate the results\n",
    "display_confusion_matrix(ytrue_train, yhat_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's accuracy based on training data\n",
    "acc_train = accuracy_score(ytrue_train, yhat_train)\n",
    "print(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ytrue and yhat based on testing data\n",
    "ytrue_test, yhat_test = get_ytrue_yhat(X_test, Y_test, model)\n",
    "\n",
    "# Visualize the confusion matrix to evaluate the results\n",
    "display_confusion_matrix(ytrue_test, yhat_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bdb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's accuracy based on testing data\n",
    "accuracy_score(ytrue_test, yhat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039eefa",
   "metadata": {},
   "source": [
    "# 8) Test model in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, landmarks, mp_draw_utils, mp_holistic):\n",
    "    \"\"\"\n",
    "    Annotates an image with facial, body, and hand landmarks. \n",
    "    \n",
    "    Arguments:\n",
    "    image -- feed from webcam represented as np.array\n",
    "    landmarks -- output from a holistic model, with positions of face, pose, and hands\n",
    "    mp_draw_utils -- MediaPipe Drawing Utils help to visualize landmarks\n",
    "    mp_holistic -- MediaPipe Holistic model that detects landmarks from an image\n",
    "    \n",
    "    Returns:\n",
    "    image -- image annotated with landmarks \n",
    "    \"\"\"\n",
    "    \n",
    "    # Draw face landmarks\n",
    "    mp_draw_utils.draw_landmarks(image = image,\n",
    "                        landmark_list = landmarks.face_landmarks,\n",
    "                        connections = mp_holistic.FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec = mp_draw_utils.DrawingSpec(color = (55, 129, 5), thickness=2, circle_radius=1),\n",
    "                        connection_drawing_spec = mp_draw_utils.DrawingSpec(color = (187, 233, 157), thickness=2, circle_radius=1))\n",
    "    \n",
    "    # Draw pose landmarks\n",
    "    mp_draw_utils.draw_landmarks(image = image,\n",
    "                        landmark_list = landmarks.pose_landmarks,\n",
    "                        connections = mp_holistic.POSE_CONNECTIONS,\n",
    "                        landmark_drawing_spec = mp_draw_utils.DrawingSpec(color = (203, 83, 46), thickness=2, circle_radius=2),\n",
    "                        connection_drawing_spec = mp_draw_utils.DrawingSpec(color = (228, 162, 142), thickness=4, circle_radius=2))\n",
    "    # Draw right hand landmarks\n",
    "    mp_draw_utils.draw_landmarks(image = image,\n",
    "                        landmark_list = landmarks.right_hand_landmarks,\n",
    "                        connections = mp_holistic.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec = mp_draw_utils.DrawingSpec(color = (0, 0, 0), thickness=2, circle_radius=2),\n",
    "                        connection_drawing_spec = mp_draw_utils.DrawingSpec(color = (105, 105, 105), thickness=3, circle_radius=2))\n",
    "    \n",
    "    # Draw left hand landmarks\n",
    "    mp_draw_utils.draw_landmarks(image = image,\n",
    "                        landmark_list = landmarks.left_hand_landmarks,\n",
    "                        connections = mp_holistic.HAND_CONNECTIONS,\n",
    "                        landmark_drawing_spec = mp_draw_utils.DrawingSpec(color = (133, 72, 148), thickness=2, circle_radius=2),\n",
    "                        connection_drawing_spec = mp_draw_utils.DrawingSpec(color = (219, 196, 225), thickness=3, circle_radius=2))\n",
    "    \n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_probs(prediction, signs, frame):\n",
    "    \"\"\"\n",
    "    Visualizes the probabilities of each predictible class.\n",
    "    All signs are stacked vertically in the left side of the frame.\n",
    "    Each sign has its own colored rectangle. The longer the rectangle, the higher the probability.\n",
    "    \n",
    "    Arguments:\n",
    "    prediction -- np.array() of predicted by model labels\n",
    "    signs -- list of signs that the model is trying to classify\n",
    "    frame -- np.array() feed from webcam\n",
    "    \n",
    "    Returns:\n",
    "    frame -- annotated frame with visualized class probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of RGB colors for each sign\n",
    "    colors = [(245,117,16), (24,24,240), (89,216,106), (225, 34, 200), (55, 55, 55)]\n",
    "    \n",
    "    # Iterate through every class and its probability\n",
    "    for index, prob in enumerate(prediction):\n",
    "        \n",
    "        # Create a colored rectangle based on sign's index and probability\n",
    "        cv2.rectangle(frame, (0, 60 + index * 40), (int(prob * 100), 90 + index * 40), colors[index], -1)\n",
    "        \n",
    "        # Add the sign's name and calculate its appropriate location \n",
    "        cv2.putText(frame, signs[index], (0, 85 + index * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_org(rect_length, rect_width, sign_prediction):\n",
    "    \"\"\"\n",
    "    Finds where to put the sign's name so that it's right in the middle of the rectangle.\n",
    "    The names of signs differ by their length, so it's required to consider that.\n",
    "    \n",
    "    Arguments:\n",
    "    rect_length -- length of the rectangle\n",
    "    rect_width -- width of the rectangle \n",
    "    sign_prediction -- name of the sign that needed to be shown in the center of a rectangle\n",
    "    \n",
    "    Returns:\n",
    "    sign_x -- value of the X-axis where the sign's name should be placed \n",
    "    sign_y -- value of the Y-axis where the sign's name should be placed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the text size based on the provided parameters \n",
    "    sign_size = cv2.getTextSize(shown_sign, cv2.FONT_HERSHEY_PLAIN, 2.5, 3)[0]\n",
    "    \n",
    "    # Calculate the appropriate coordinates to place the given text  \n",
    "    sign_x = (rect_length - int(1.5* sign_size[0])) // 2\n",
    "    sign_y = (rect_width + sign_size[1]) // 2\n",
    "\n",
    "    return sign_x, sign_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MediaPipe Holistic, which detects landmarks on a detected person\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Set MediaPipe drawing utils, which draws bounding boxes and keypoints on the image\n",
    "mp_draw_utils = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb36dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a list of frames, which will form a sentence\n",
    "sequence = []\n",
    "\n",
    "# Store predictions made by a trained model on a provided sentence \n",
    "predictions = []\n",
    "\n",
    "# Set up a lower limit of prediction confidence\n",
    "threshold = 0.85\n",
    "\n",
    "# Store information about the most recent prediction\n",
    "sign_prediction = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c744bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the capturing device \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set up the MediaPipe Holistic model\n",
    "with mp.solutions.holistic.Holistic(min_detection_confidence=0.75, min_tracking_confidence=0.75) as holistic:\n",
    "    \n",
    "    # Until the capturing device is not shut down\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Get landmarks from the frame\n",
    "        landmarks = process_image(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        frame = draw_landmarks(frame, landmarks, mp_draw_utils, mp_holistic)\n",
    "        \n",
    "        # Extract hand landmarks and append them to the sequence list in the correct order\n",
    "        keypoints = extract_hand_landmarks(landmarks)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        # If enough frames were gathered to form a sentence\n",
    "        if len(sequence) == 30:\n",
    "            \n",
    "            # Make a prediction and get the index with the highest probability\n",
    "            prediction = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            prediction_index = np.argmax(prediction)\n",
    "            predictions.append(prediction_index)\n",
    "            \n",
    "            # If the last 15 predictions have the same label \n",
    "            if np.all(np.array(predictions[-15:])==prediction_index):\n",
    "                \n",
    "                # And their probability passes the confidence level \n",
    "                if prediction[prediction_index] > threshold:\n",
    "                    \n",
    "                    # And it is not the most recent predicted sign\n",
    "                    if signs[prediction_index] != sign_prediction:\n",
    "                        \n",
    "                        # Then assign a new final prediction \n",
    "                        sign_prediction = signs[prediction_index] if signs[prediction_index] != \"Nothing\" else \"\"\n",
    "    \n",
    "            # Vizualize the probabilities of each gesture\n",
    "            frame = show_probs(prediction, signs, frame)\n",
    "              \n",
    "        # Calculate the location of the last predicted action and correspondingly annotate the image        \n",
    "        sign_x, sign_y = calculate_org(640, 40, sign_prediction)\n",
    "        cv2.rectangle(frame, (0,0), (640, 40), (0, 171, 255), -1)\n",
    "        cv2.putText(frame, ' '.join(sign_prediction), (sign_x, sign_y), \n",
    "                       cv2.FONT_HERSHEY_PLAIN, 2.5, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "        \n",
    "        # Create a window to output the webcam feed\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "        # Break from a loop by pressing 'q' if necessary\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the capturing device and destroy the webcam output window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
